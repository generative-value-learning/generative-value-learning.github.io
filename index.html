<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta
      name="description"
      content="Generative Value Learning: Vision Language Models are In-Context Value Learners"
    />
    <meta
      name="keywords"
      content="GVL, value learning, vision language models, VLM, robotics, reinforcement learning"
    />
    <meta
      property="og:image"
      content="./static/images/icv/method.png"
    />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>
      Generative Value Learning: Vision Language Models are In-Context Value Learners
    </title>

    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="Generative Value Learning" />
    <meta
      name="twitter:description"
      content="Vision Language Models are In-Context Value Learners"
    />
    <meta
      name="twitter:image"
      content="./static/images/icv/method.png"
    />

    <link
      href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
      rel="stylesheet"
    />

    <link rel="stylesheet" href="./static/css/bulma.min.css" />
    <link rel="stylesheet" href="./static/css/bulma-carousel.min.css" />
    <link rel="stylesheet" href="./static/css/bulma-slider.min.css" />
    <link rel="stylesheet" href="./static/css/fontawesome.all.min.css" />
    <link
      rel="stylesheet"
      href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css"
    />
    <link rel="stylesheet" href="./static/css/index.css" />
    <link rel="icon" href="./static/images/favicon.svg" />

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script defer src="./static/js/fontawesome.all.min.js"></script>
    <script src="./static/js/bulma-carousel.min.js"></script>
    <script src="./static/js/bulma-slider.min.js"></script>
    <script src="./static/js/index.js"></script>
    <style>
      /* body {
        font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, sans-serif;
        max-width: 800px;
        margin: 40px auto;
        padding: 20px;
        background: #ffffff;
        color: #333333;
      } */
      
      table {
        width: 100%;
        border-collapse: collapse;
        margin: 20px 0;
        font-size: 15px;
        background: white;
        box-shadow: 0 1px 3px rgba(0,0,0,0.1);
      }
      
      th {
        border-top: 1.5px solid #000;
        border-bottom: 1.5px solid #000;
        padding: 12px 24px;
        text-align: left;
        font-weight: 600;
        font-size: 16px;
      }
      
      td {
        padding: 12px 24px;
      }
      
      tr:last-child td {
        border-bottom: 1.5px solid #000;
      }
      
      tr:hover {
        background-color: #f5f5f5;
      }
      
      .method-group {
        border-bottom: 1.5px solid #000;
      }
      
      .dataset-name, .method-name {
        font-weight: 500;
      }
      
      .citation {
        color: #FF69B4;
      }
      
      .metric {
        text-align: right;
        font-feature-settings: "tnum";
        font-variant-numeric: tabular-nums;
      }
      
      .bold-metric {
        font-weight: bold;
      }
      
      .year {
        color: #666;
      }
      
      .sub-method {
        padding-left: 24px;
      }

      .demo-container {
        display: flex;
        gap: 20px;
        margin-bottom: 20px;
      }
      
      .task-select {
        flex: 1;
        padding: 12px;
        border: 2px solid #000;
        background: white;
        appearance: none;
        position: relative;
        cursor: pointer;
      }
      
      .status-container {
        flex: 1;
        border: 2px solid #000;
        display: flex;
        overflow: hidden;
      }
      
      .status {
        flex: 1;
        padding: 12px;
        text-align: center;
      }
      
      .success {
        background: #000;
        color: white;
      }
      
      .fail {
        background: white;
        color: black;
      }
      
      .video-area {
        width: 80%;
        margin: 0 auto;
        aspect-ratio: 16/9;
        background: #e0e0e0;
        border: 2px solid #000;
        position: relative;
      }

      .video-overlay {
        position: absolute;
        bottom: 60px;
        left: 20px;
        background: rgba(0, 0, 0, 0.7);
        color: white;
        padding: 8px 12px;
        border-radius: 4px;
        font-size: 14px;
      }
    </style>
  </head>
  <body>
    <section class="hero">
      <div class="hero-body">
        <div class="container is-max-desktop">
          <div class="columns is-centered">
            <div class="column has-text-centered">
              <h1 class="title is-1 publication-title">
                Vision Language Models are  
                In-Context Value Learners
              </h1>
              <div class="is-size-5 publication-authors">
                <span class="author-block">
                  <a href="https://jasonma2016.github.io/">Jason Ma</a
                  ><sup>1,2,†</sup>,</span
                >
                <span class="author-block">
                  <a href="http://joeyhejna.com/">Joey Hejna</a
                  ><sup>1,3</sup>,</span
                >
                <span class="author-block">
                  <a href="">Ayzaan Wahid</a
                  ><sup>1</sup>,</span
                >
                <span class="author-block">
                  <a href="">Chuyuan Fu</a
                  ><sup>1</sup>,</span
                >
                <span class="author-block"></span>
                  <a href="">Dhruv Shah</a
                  ><sup>1</sup>,</span
                >
                <span class="author-block"></span>
                  <a href="">Jacky Liang</a
                  ><sup>1</sup>,</span
                >
                <span class="author-block"></span>
                  <a href="">Zhuo Xu</a
                  ><sup>1</sup>,</span
                >
                <span class="author-block"></span>
                  <a href="https://kirmani.ai/">Sean Kirmani</a
                  ><sup>1</sup>,</span
                >
                <span class="author-block"></span>
                  <a href="">Peng Xu</a
                  ><sup>1</sup>,</span
                >
                <span class="author-block">
                  <a href="https://dannydriess.github.io/">Danny Driess</a
                  ><sup>1</sup>,
                </span>
                <span class="author-block"></span>
                  <a href="https://tedxiao.me/">Ted Xiao</a
                  ><sup>1</sup>,
                </span>
                <span class="author-block"></span>
                  <a href="https://jonathantompson.github.io/">Jonathan Tompson</a
                  ><sup>1</sup>,
                </span>
                <span class="author-block"></span>
                  <a href="https://obastani.github.io/">Osbert Bastani</a
                  ><sup>2</sup>,
                </span>
                <span class="author-block"></span>
                  <a href="https://www.seas.upenn.edu/~dineshj/">Dinesh Jayaraman</a
                  ><sup>2</sup>,
                </span>
                <span class="author-block"></span>
                  <a href="https://tedxiao.me/">Wenhao Yu</a
                  ><sup>1</sup>,
                </span>
                <span class="author-block"></span>
                  <a href="https://tedxiao.me/">Tingnan Zhang</a
                  ><sup>1</sup>,
                </span>
                <span class="author-block">
                  <a href="https://dorsa.fyi/">Dorsa Sadigh</a><sup>1,3</sup>,
                </span>
                <span class="author-block">
                  <a href="https://fxia22.github.io/">Fei Xia</a><sup>1</sup>
                </span>
              </div>
              <div>
                <sup>†</sup> Work done while interning at Google DeepMind.
              </div>

              <br />
              <div class="is-size-5 publication-authors">
                <ul class="affiliation-list">
                  <li class="affiliation">
                    <img
                      src="./static/images/deepmind.jpg"
                      alt="DeepMind Logo"
                      class="logo"
                    />
                    <span class="author-block"
                      ><sup>1</sup>Google DeepMind</span
                    >
                  </li>
                  <li class="affiliation">
                    <img
                      src="./static/images/penn.jpg"
                      alt="Penn Logo"
                      class="logo"
                    />
                    <span class="author-block"><sup>2</sup>University of Pennsylvania</span>
                  </li>
                  <li class="affiliation">
                    <img
                      src="./static/images/stanford.jpg"
                      alt="Stanford Logo"
                      class="logo"
                    />
                    <span class="author-block"
                      ><sup>3</sup>Stanford University</span
                    >
                  </li>
                </ul>
                <br>
                <!-- <p> <b>CVPR 2024 </b></p> -->
              </div>
              <br />
              <div class="column has-text-centered">
                <div class="publication-links">
                  <!-- PDF Link. -->
                  <span class="link-block">
                    <a
                      href="./gvl.pdf"
                      class="external-link button is-normal is-rounded is-dark"
                    >
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Paper</span>
                    </a>
                  </span>
                  <span class="link-block">
                    <a
                      href="#online-demo"
                      class="external-link button is-normal is-rounded is-dark"
                    >
                      <span class="icon">
                        <i class="fas fa-robot"></i>
                      </span>
                      <span>Online Demo</span>
                    </a>
                  </span>
                  <!-- Video Link. -->
                  <!-- <span class="link-block">
                    <a
                      href="https://www.youtube.com/watch?v=_z9b5E_obbE"
                      class="external-link button is-normal is-rounded is-dark"
                    >
                      <span class="icon">
                        <i class="fab fa-youtube"></i>
                      </span>
                      <span>Video</span>
                    </a>
                  </span> -->
                  <!-- <span class="link-block">
                    <a
                      href="#community-implementation"
                      class="external-link button is-normal is-rounded is-dark"
                    >
                      <span class="icon">
                        <i class="fas fa-code"></i>
                      </span>
                      <span>Code (3rd party)</span>
                    </a>
                  </span> -->
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </section>
    <section class="section">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column is-full-width has-text-justified">
            <h2 class="title is-3 has-text-centered">Interactive Demo</h2>
            <div class="content has-text-centered">
              <iframe 
                src="./interactive/json-frame-viewer.html"
                width="100%" 
                style="min-height: 700px;"
                frameborder="0"
                scrolling="no"
                onload="this.style.height=this.contentWindow.document.documentElement.scrollHeight+'px';">
              </iframe>
            </div>
          </div>
        </div>
      </div>
    </section>

    <section class="section">
      <div class="container is-max-desktop">
        <!-- Abstract. -->
        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <h2 class="title is-3">Abstract</h2>
            <div class="content has-text-justified">
              Predicting temporal progress from visual trajectories is important for intelligent robots that can learn, adapt, and improve.
              
              However, learning such progress estimator, or temporal value function, across different tasks and domains requires both a large amount of diverse data and methods which can scale and generalize.
              
              To address these challenges, we present Generative Value Learning (GVL), a universal value function estimator that leverages the world knowledge embedded in vision-language models (VLMs) to predict task progress.
              
              Naively asking a VLM to predict values for a video sequence performs poorly due to the strong temporal correlation between successive frames. Instead, GVL poses value estimation as a temporal ordering problem over shuffled video frames; this seemingly more challenging task encourages VLMs to more fully exploit their underlying semantic and temporal grounding capabilities to differentiate frames based on their perceived task progress, consequently producing significantly better value predictions.
              
              Without any robot or task specific training, GVL can in-context zero-shot and few-shot predict effective values for more than 300 distinct real-world tasks across diverse robot platforms, including challenging bimanual manipulation tasks.
              
              Furthermore, we demonstrate that GVL permits flexible multi-modal in-context learning via examples from heterogeneous tasks and embodiments, such as human videos.
              
              The generality of GVL enables various downstream applications pertinent to visuomotor policy learning, including dataset filtering, success detection, and value-weighted regression -- all without any model training or finetuning.
              <!--/ Abstract. -->
            </div>
          </div>
        </div>
      </div>
    </section>

    <div class="hr"></div>

    <section class="section">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column is-full-width has-text-justified">
            <h2 class="title is-3 has-text-centered">Method Overview</h2>

            <h3 class="title is-4">In-Context Value Learning as Temporal Ordering </h3>
            <div class="content has-text-justified">
              <p>
                GVL auto-regressively predicts task completion percentage over shuffled frames, enabling impressive
in-context value learning. GVL can effectively zero-shot and few-shot predict task progress on diverse and
challenging real-world tasks; these capabilities enable expansive set of downstream applications, including
dataset filtering, success detection, and policy learning.
              </p>
            </div>
            <div class="content has-text-centered">
              <video
                class="inline-figure-six"
                autoplay
                loop
                muted
                >
                <source src="./static/videos/concept.mp4" type="video/mp4">
              </video>
            </div>
            <!-- <h3 class="title is-4">Zero-Shot Value Predictions on 300+ tasks, 20 embodiments, and 50 datasets</h3>
            <div class="content has-text-justified">
              <p>
                TODO.
              </p>
            </div> -->
            <!-- <div class="content has-text-centered">
              <div class="columns is-centered">
                <div class="column">
                  <object
                    data="./static/images/icv/icv_01_shot.pdf"
                    type="application/pdf"
                    class="inline-figure-six"
                    alt="Samples of the dataset."
                  >
                    <p>Your browser does not support PDFs. Please download the PDF to view it.</p>
                  </object>
                </div>
                <div class="column">
                  <object
                    data="./static/images/icv/icv_shots_average.pdf"
                    type="application/pdf" 
                    class="inline-figure-six"
                    alt="Samples of the dataset."
                  >
                    <p>Your browser does not support PDFs. Please download the PDF to view it.</p>
                  </object>
                </div>
              </div>
            </div> -->
            <!-- <h3 class="title is-4">Chain-of-thought Spatial Reasoning</h3>
            <div class="content has-text-justified">
              <p>
                With the ability to perform direct spatial reasoning like
                humans, we can let SpatialVLM perform Chain-of-Thought Spatial
                reasoning by letting it talk with with an LLM. As we will show
                later in experiments section, the direct reasoning capabilities,
                when combined with chain-of-thought reasoning can answer many
                multi-step questions.
              </p>
            </div> -->
          </div>
        </div>
      </div>
    </section>

    <div class="hr"></div>

    <section class="section">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column is-full-width has-text-justified">
            <h2 class="title is-3 has-text-centered">Experiments</h2>

            <div class="content has-text-justified">
              <p>
                Through extensive benchmark, we found our proposed framework can
                significantly enhance the ability of visual language models in
                performing different types of spatial reasoning like humans, as
                well as unlocking novel downstream applications such as
                robotics.
              </p>
            </div>
            <h3 class="title is-4">Evaluation Metric: Value-Order Correlation</h3>

            <div class="content has-text-justified">
              <p>
                We introduce a lightweight, yet predictive method for evaluating value models at scale on real-world robotic datasets: Value-Order
Correlation (VOC). This metric computes the rank correlation between the predicted values and the
chronological order of the input expert video:
              </p>
            </div>
            <div class="content has-text-centered">
              <img
                src="./static/images/icv/voc.png"
                class="inline-figure-six"
                alt="value-order correlation."
                height="auto"
                width="600px"
              />
            </div>
            <div class="content has-text-justified">
              <p>
                VOC ranges from$−1 to 1, where 1 indicates that the two orderings are perfectly aligned. Expert
                quality demonstrations, by construction, have values that monotonically increase with time, and
                thus a good value model should have high VOC scores when evaluated on expert videos. On the
                other hand, fixing a good value model, low-quality trajectories should have low VOC scores. This
                is because sub-optimal trajectories often contain high repetition of visually similar frames due to
                the presence of redundant, re-attempt actions or poorly-placed cameras. As such, the values along
                the trajectories should not be monotonic, resulting in low correlation with the ground-truth timestep
                order.              </p>
            </div>
            <h3 class="title is-4">Zero-Shot Value Predictions on 300+ tasks, 20 embodiments, and 50 datasets</h3>

            <div class="content has-text-justified">
              <p>
TODO.
              </p>
            </div>

          
            <h3 class="title is-4">GVL Demonstrates Multi-Modal In-Context Learning Capabilities</h3>

            <div class="content has-text-justified">
              <p>
                GVL demonstrates appealing in-context scaling as the average VOC
score steadily improves as we increase the number of in-context examples. Even with 5 in-context
trajectories, meaning 150 total shuffled images, GVL is able to utilize its full context and exhibit
strong generalization. This result demonstrates how state-of-art long-context-window VLMs, such as
Gemini-1.5-Pro, can be re-purposed to make for general-purpose value functions with impressive test-time improvement capability, quickly mastering value predictions with minimal supervision.              </p>
            </div>
            <div class="content has-text-centered">
              <img
                src="./static/images/icv/icl.png"
                class="inline-figure-six"
                alt="in-context learning."
              />
            </div>

            <h3 class="title is-4">Cross-Embodiment and Cross-Task In-Context Learning</h3>
            <div class="content has-text-justified">
              <p>
                Examples in-context are not limited to robot demon-strations. One advantage of GVL is that it can still benefit from in-context learning even when thedemonstrations come from a different embodiment. Specifically, we record humans performing thesame tasks as the ALOHA robot demonstrations and then use these human demonstrations as in-context examples for value prediction.
                As shown, GVL with one cross-embodiment in-contextexample can effectively improve over its zero-shot counterpart. In the Appendix, we also show thatGVL can similarly benefit fromcross-taskin-context learning. In conclusion, GVL presents a versatileframework for in-context value learning that can scale up to even the most challenging manipulation tasks.
              </p>
            </div>
            <div class="content has-text-centered">
              <img
                src="./static/images/icv/human_videos.png"
                class="inline-figure-six"
                alt="chain of thought spatial reasoning"
              />
            </div>
          </div>
        </div>
      </div>
    </section>

    <div class="hr"></div>

    <section class="section">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column is-full-width has-text-justified">
            <h2 class="title is-3 has-text-centered">Downstream Applications</h2>
            <h3 class="title is-4">Dataset Quality Estimation</h3>
            <div class="content has-text-justified">
              <p>
                We demonstrate that GVL's VOC scores can be used to estimate dataset quality. For each dataset in OXE,
                we compute the average VOC score for its sampled trajectories and present the ranking of the average scores.
                The full results are presented in paper Appendix. Here, we present a subset of selected representative large-scale datasets in
                OXE. We see that datasets have large spread in their VOC scores, but these scores are interpretable
                and match human intuitions. Specifically, datasets collected from human teleoperators with relative
                fixed camera placements, such as RT-1 (Brohan et al., 2022), Dobb-E (Shafiullah et al., 2023), and
                Bridge (Ebert et al., 2021; Walke et al., 2023), have high VOC scores, despite their diversity in
                scenes and tasks. In contrast, datasets with autonomous data collection via scripted motions or motor
                babbling, such as QT-OPT (Kalashnikov et al., 2018) and RoboNet (Dasari et al., 2019), contain high
                number of suboptimal trajectories that do not exhibit smooth temporal structure to be re-shuffled.
              </p>
            </div>
            <div class="content has-text-centered">
             
              <table>
                <thead>
                  <tr>
                    <th>Dataset</th>
                    <th style="text-align: right">Avg. VOC</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td><span class="dataset-name">RT-1</span> <span class="citation">(Brohan et al., 2022)</span></td>
                    <td class="metric">0.74</td>
                  </tr>
                  <tr>
                    <td><span class="dataset-name">Dobb-E</span> <span class="citation">(Shafiullah et al., 2023)</span></td>
                    <td class="metric">0.53</td>
                  </tr>
                  <tr>
                    <td><span class="dataset-name">Bridge</span> <span class="citation">(Walke et al., 2023)</span></td>
                    <td class="metric">0.51</td>
                  </tr>
                  <tr>
                    <td><span class="dataset-name">QT-OPT</span> <span class="citation">(Kalashnikov et al., 2018)</span></td>
                    <td class="metric">0.19</td>
                  </tr>
                  <tr>
                    <td><span class="dataset-name">DROID</span> <span class="citation">(Khazatsky et al., 2024)</span></td>
                    <td class="metric">-0.01</td>
                  </tr>
                  <tr>
                    <td><span class="dataset-name">RoboNet</span> <span class="citation">(Dasari et al., 2019)</span></td>
                    <td class="metric">-0.85</td>
                  </tr>
                </tbody>
              </table>
            </div>

            <h3 class="title is-4">Success Detection and Filtered BC</h3>
            <div class="content has-text-justified">
              <p>
              The VOC score can be used as a threshold score for success detection. The resulting success detection method, GVL-SD, substantially outperforms SuccessVQA using the same VLM on all evaluation metrics. Furthermore, filtered BC with GVL-SD always outperforms the base imitation learning algorithm (ACT) regardless of the threshold value.
              </p>
            </div>
            <div class="content has-text-centered">
             
              <body>
                <table>
                  <thead>
                    <tr>
                      <th>Method</th>
                      <th class="metric">Accuracy</th>
                      <th class="metric">Precision</th>
                      <th class="metric">Recall</th>
                    </tr>
                  </thead>
                  <tbody>
                    <tr class="method-group">
                      <td class="method-name">GVL-SD (Zero-Shot)</td>
                      <td class="metric">0.71</td>
                      <td class="metric">0.71</td>
                      <td class="metric">0.71</td>
                    </tr>
                    <tr class="method-group">
                      <td class="method-name">GVL-SD (One-Shot)</td>
                      <td class="metric bold-metric">0.75</td>
                      <td class="metric bold-metric">0.85</td>
                      <td class="metric">0.70</td>
                    </tr>
                    <tr>
                      <td class="method-name">SuccessVQA <span class="citation">(Du et al., 2023)</span></td>
                      <td class="metric">0.62</td>
                      <td class="metric">0.33</td>
                      <td class="metric">0.73</td>
                    </tr>
                    <tr>
                      <td class="method-name">SuccessVQA-CoT</td>
                      <td class="metric">0.63</td>
                      <td class="metric">0.44</td>
                      <td class="metric">0.68</td>
                    </tr>
                  </tbody>
                </table>
              </body>
              <img
              src="./static/images/icv/filtered_bc.png"
              class="inline-figure-four-thirds"
              alt="Comparison with baselines."
            />
            </div>
            <div class="content has-text-justified">
              <p>
                Qualitatively, we indeed see that GVL assigns much higher scores to successful trajectories than failure trajectories; in contrast, removing the shuffling mechanism in GVL significantly reduces its discriminability on failure trajectories.
              </p>
            </div>

            <div class="content has-text-centered">
              <img
                src="./static/images/icv/success_detection_sim.png"
                class="inline-figure-four-thirds"
                alt="Comparison with baselines."
              />
            </div>
            <h3 class="title is-4">Real-World Advantage-Weighted Regression</h3>

            <div class="content has-text-justified">
              <p>
                We illustrate how
                GVL can assign importance weights to individual transitions within trajectories at a fine-grained
                level akin to offline reinforcement learning. For these experiments we use real-world demonstration
                data collected by human teleoperation on bi-manual ALOHA robot setups. Unlike simulation, our
                datasets only contain successful task executions but can be sub-optimal and multi-modal. Thus, we
                directly utilize GVL’s values with advantage weighted regression (AWR) (Peters & Schaal, 2007;
                Peng et al., 2019), in which we weight each individual transition by the estimated advantange, or
                GVL value difference for that step:
              </p>
            </div>
            <div class="content has-text-centered">
              <img
                src="./static/images/icv/awr.png"
                class="inline-figure-four-thirds"
                alt="reward heatmap."
                height="auto"
                width="500px"
              />
             <table>
    <thead>
      <tr>
        <th>Real-World ALOHA Tasks</th>
        <th class="metric">GVL + DP</th>
        <th class="metric">DP</th>
        <th class="metric">Avg. VOC</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td class="task-name">bowl-in-rack</td>
        <td class="metric bold-metric">7/10</td>
        <td class="metric">6/10</td>
        <td class="metric">0.57</td>
      </tr>
      <tr>
        <td class="task-name">banana-handover</td>
        <td class="metric bold-metric">7/10</td>
        <td class="metric">5/10</td>
        <td class="metric">0.73</td>
      </tr>
      <tr>
        <td class="task-name">close-laptop</td>
        <td class="metric bold-metric">9/10</td>
        <td class="metric">6.5/10</td>
        <td class="metric">0.59</td>
      </tr>
      <tr>
        <td class="task-name">open-drawer</td>
        <td class="metric">4/10</td>
        <td class="metric bold-metric">6/10</td>
        <td class="metric">0.09</td>
      </tr>
      <tr>
        <td class="task-name">remove-gears</td>
        <td class="metric">4.67/10</td>
        <td class="metric bold-metric">7/10</td>
        <td class="metric">0.19</td>
      </tr>
      <tr>
        <td class="task-name">pen-handover</td>
        <td class="metric bold-metric">1.5/10</td>
        <td class="metric">0/10</td>
        <td class="metric">0.43</td>
      </tr>
      <tr>
        <td class="task-name">fold-dress</td>
        <td class="metric bold-metric">7/10</td>
        <td class="metric bold-metric">7/10</td>
        <td class="metric">0.66</td>
      </tr>
    </tbody>
  </table>
            <img
            src="./static/images/icv/real_world_tasks.png"
            class="inline-figure-four-thirds"
            alt="reward heatmap."
            height="auto"
            width="700px"
          />
            </div>
            <div class="content has-text-justified">
              <p>
              We use diffusion policy (DP) as the policy backbone (Chi et al., 2023) for each task, and compare
              training diffusion policies with GVL (One-Shot) advantage weighting or lack thereof. We evaluate on
              7 tasks with 10 trials per task and report success rate in Table 3. As can be seen, on a majority tasks,
              GVL-DP outperforms DP and we see a clear correlation between improvement over DP and the VOC
              score. That is, when the value predictions are of high quality as judged by VOC, policy learning
              can benefit from GVL value weighting. On open-drawer and remove-gears, the top-down
              view does not provide sufficient resolution to distinguish task progress (see Fig. 8), as a consequence,
              the value predictions can be noisy, which can hurt policy learning. However, given the in-context
              learning results, we believe that it is possible to improve policy learning even on difficult tasks with
              non-ideal camera viewpoints.   
                </p>
            </div>
          </div>
        </div>

        <h3 class="title is-4">Qualitative Results</h3>
        <div class="content has-text-justified">
          <p>
            We show qualitative results on several robotic manipulation tasks below. For each task, we show both successful and failed attempts.
          
         

          <div class="demo-container">
            <select class="task-select" id="taskSelect">
              <option value="fold-dress">Fold Dress</option>
              <option value="open-drawer">Open Drawer</option>
              <option value="banana-handover">Banana Handover</option>
              <option value="remove-gears">Remove Gears</option>
              <option value="close-laptop">Close Laptop</option>
              <option value="open-jar-lid">Open Jar Lid</option>
            </select>
            <div class="status-container">
              <div class="status success">Success</div>
              <div class="status fail">Fail</div>
            </div>
          </div>
          
          <div class="video-area" id="videoArea">
            <video id="demoVideo" width="100%" controls autoplay muted loop>
              <source src="./static/videos/fold-dress/success.webm" type="video/webm">
            </video>
            <div class="video-overlay">
              autonomous, 1x speed
            </div>
          </div>

          <script>
            const taskSelect = document.getElementById('taskSelect');
            const demoVideo = document.getElementById('demoVideo');
            const successStatus = document.querySelector('.success');
            const failStatus = document.querySelector('.fail');

            taskSelect.addEventListener('change', (e) => {
              const task = e.target.value;
              demoVideo.src = `./static/videos/${task}/success.webm`;
              // Reset status styling to success state when task changes
              successStatus.style.background = '#000';
              successStatus.style.color = 'white';
              failStatus.style.background = 'white'; 
              failStatus.style.color = 'black';
            });

            successStatus.addEventListener('click', () => {
              const task = taskSelect.value;
              demoVideo.src = `./static/videos/${task}/success.webm`;
              successStatus.style.background = '#000';
              successStatus.style.color = 'white';
              failStatus.style.background = 'white';
              failStatus.style.color = 'black';
            });

            failStatus.addEventListener('click', () => {
              const task = taskSelect.value;
              demoVideo.src = `./static/videos/${task}/fail.webm`;
              failStatus.style.background = '#000';
              failStatus.style.color = 'white';
              successStatus.style.background = 'white';
              successStatus.style.color = 'black';
            });
          </script>
        </div>

      </div>
      
    </section>


    <section class="section">
        <div class="container">
          <div class="columns is-centered">
            <div class="column is-full-width has-text-justified">
              <h2 id="online-demo" class="title is-3 has-text-centered">Online Demo</h2>
              <head>
                <meta charset="UTF-8">
                <meta name="viewport" content="width=device-width, initial-scale=1.0">
                <title>GVL Live Demo</title>
                <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
                <style>
                    .gvl-demo-container {
                        font-family: system-ui, sans-serif;
                        max-width: 1000px;
                        margin: 2rem auto;
                        padding: 1rem;
                        border: 1px solid #e5e7eb;
                        border-radius: 8px;
                        box-shadow: 0 1px 3px rgba(0,0,0,0.1);
                    }
                    .gvl-flex-container {
                        display: flex;
                        flex-direction: column;
                        gap: 1rem;
                    }
                    .gvl-input-group {
                        display: flex;
                        flex-direction: column;
                        gap: 0.5rem;
                    }
                    .gvl-input, .gvl-textarea {
                        padding: 0.5rem;
                        border: 1px solid #ccc;
                        border-radius: 4px;
                        font-size: 1rem;
                    }
                    .gvl-button {
                        background: #0066ff;
                        color: white;
                        border: none;
                        padding: 0.75rem 1.5rem;
                        border-radius: 4px;
                        cursor: pointer;
                        font-size: 1rem;
                    }
                    .gvl-button:hover {
                        background: #0052cc;
                    }
                    .gvl-button:disabled {
                        background: #ccc;
                        cursor: not-allowed;
                    }
                    .gvl-result {
                        margin-top: 1rem;
                        padding: 1rem;
                        border: 1px solid #ccc;
                        border-radius: 4px;
                        min-height: 100px;
                        white-space: pre-wrap;
                    }
                    .gvl-error {
                        color: #dc2626;
                        display: none;
                        margin-top: 1rem;
                    }
                    #gvl-image-preview {
                        max-width: 100%;
                        max-height: 300px;
                        margin-top: 1rem;
                        display: none;
                        object-fit: contain;
                    }
                    .gvl-progress {
                        display: none;
                        margin: 0.5rem 0;
                    }
                    .gvl-frame-container {
                        position: relative;
                        display: inline-block;
                        margin: 1rem;
                    }
                    .gvl-frame-number {
                        position: absolute;
                        left: 5px;
                        top: 5px;
                        background: rgba(40, 167, 69, 0.5);
                        color: white;
                        padding: 2px 6px;
                        border-radius: 4px;
                        font-size: 12px;
                        transition: background 0.3s;
                    }
                    .gvl-frame-container:hover .gvl-frame-number {
                        background: rgba(40, 167, 69, 0.7);
                    }
                    .gvl-shuffled-frame-number {
                        position: absolute;
                        right: 5px;
                        top: 5px;
                        background: rgba(255, 200, 200, 0.5);
                        color: black;
                        padding: 2px 6px;
                        border-radius: 4px;
                        font-size: 12px;
                        transition: background 0.3s;
                    }
                    .gvl-frame-container:hover .gvl-shuffled-frame-number {
                        background: rgba(255, 200, 200, 0.9);
                    }
                    .gvl-order-buttons {
                        margin-top: 0.5rem;
                        display: flex;
                        gap: 0.5rem;
                    }
                    .gvl-frame-progress {
                        position: absolute;
                        right: -10px;
                        top: 0;
                        width: 4px;
                        background: #eee;
                        height: 100%;
                    }
                    .gvl-frame-progress-fill {
                        position: absolute;
                        bottom: 0;
                        width: 100%;
                        background: #28a745;
                        transition: height 0.3s ease;
                    }
                    .gvl-frame-description {
                        position: absolute;
                        left: 5px;
                        bottom: 5px;
                        background: rgba(0, 0, 0, 0.3);
                        color: white;
                        padding: 2px 6px;
                        border-radius: 4px;
                        font-size: 10px;
                        max-width: calc(100% - 20px);
                        transition: background 0.3s;
                    }
                    .gvl-frame-container:hover .gvl-frame-description {
                        background: rgba(0, 0, 0, 0.7);
                    }
                    #gvl-plot-container {
                        width: 100%;
                        height: 400px;
                        margin-top: 1rem;
                        display: none;
                    }
                </style>
            </head>
            <body>
                <div class="gvl-demo-container">
                    <div class="gvl-flex-container">
                        <div class="gvl-instructions">
                          <p> This video does zero-shot video unshuffling and task progress estimation, following this paper.</p>
                            <p>To try this demo, first upload a video enter your Gemini API key and task description, then first shuffle it, and "Get Response" to analyze the frames. 
                            </p>
                            <p>After receiving the response, you can click "Parse Response" to see the predicted task completion percentage for each frame. You can toggle back to GT order to examine the predicted value function as well as the caption.</p>
                        </div>
                        <div class="gvl-input-group">
                            <label for="gvl-api-key">API Key: <a href="https://aistudio.google.com/app/u/1/apikey" target="_blank" style="font-size: 0.8em;">(Get API key here)</a></label>
                            <input type="password" id="gvl-api-key" class="gvl-input" placeholder="Enter your API key">
                        </div>
            
                        <div class="gvl-input-group">
                            <label for="gvl-task-description">Task Description:</label>
                            <textarea id="gvl-task-description" class="gvl-textarea" rows="2" placeholder="Describe the robot task"></textarea>
                        </div>
            
                        <div class="gvl-input-group">
                            <label for="gvl-image-input">Upload Video:</label>
                            <div style="margin-bottom: 0.5rem; font-size: 0.9rem; color: #666;">
                                <span style="color: #28a745;">Green numbers</span> show ground truth frame order. 
                                <span style="color: #dc3545;">Red numbers</span> show shuffled frame order.
                            </div>
                            <input type="file" id="gvl-image-input" accept="video/*">
                            <img id="gvl-image-preview" alt="Preview">
                            <div id="gvl-frame-container" style="display: none; margin-top: 1rem;"></div>
                            <div class="gvl-order-buttons" id="gvl-order-buttons" style="display: none;">
                                <button onclick="orderFrames('gt')" class="gvl-button">Order by Ground Truth</button>
                                <button onclick="orderFrames('shuffled')" class="gvl-button">Order by Shuffled</button>
                            </div>
                        </div>
            
                        <button id="gvl-submit-btn" onclick="sendRequest()" disabled class="gvl-button">Get Response (Running Gemini 1.5 Pro)</button>
                        <button id="gvl-parse-btn" onclick="parseResponse()" style="margin-top: 0.5rem;" disabled class="gvl-button">Parse Response</button>
                        <button id="gvl-download-btn" onclick="downloadPrompt()" style="margin-top: 0.5rem;" disabled class="gvl-button">Download Prompt</button>
            
                        <div id="gvl-progress" class="gvl-progress">Processing request...</div>
                        <div id="gvl-error" class="gvl-error"></div>
                        <div id="gvl-result" class="gvl-result"></div>
                        <div id="gvl-plot-container"></div>
                    </div>
                </div>
            
                <script>
                    const imageInput = document.getElementById('gvl-image-input');
                    const imagePreview = document.getElementById('gvl-image-preview');
                    const frameContainer = document.getElementById('gvl-frame-container');
                    const submitBtn = document.getElementById('gvl-submit-btn');
                    const parseBtn = document.getElementById('gvl-parse-btn');
                    const downloadBtn = document.getElementById('gvl-download-btn');
                    const orderButtons = document.getElementById('gvl-order-buttons');
                    const plotContainer = document.getElementById('gvl-plot-container');
                    let imageBase64 = null;
                    let imageType = null;
                    let frames = [];
                    let isOrdered = true; // Start with GT order

                    function preparePromptData() {
                        const taskDescription = document.getElementById('gvl-task-description').value;
                        const frameImages = Array.from(frameContainer.getElementsByTagName('img'));
                        
                        if (!taskDescription) {
                            throw new Error('Please provide task description');
                        }

                        if (frameImages.length === 0) {
                            throw new Error('Please provide video frames');
                        }

                        if (isOrdered) {
                            throw new Error('Frames must be in shuffled order');
                        }

                        const promptText = `You are an expert roboticist tasked to predict task completion percentages for frames of a robot for the task of ${taskDescription}. The task completion percentages are between 0 and 100, where 100 corresponds to full task completion. We provide several examples of the robot performing the task at various stages and their corresponding task completion percentages. Note that these frames are in random order, so please pay attention to the individual frames when reasoning about task completion percentage.

Initial robot scene:
In the initial robot scene, the task completion percentage is 0.

Now, for the task of *${taskDescription}*, output the task completion percentage for the following frames that are presented in random order. Format your response as follow in json format, make sure to include all frames:

[
{"frame_number": i, "frame_description": "...", "task_completion_percentage": (0-100)}
]`;

                        const parts = [{ text: promptText }];
                        frameImages.forEach((frameImg, i) => {
                            parts.push({
                                text: `Frame ${i + 1}:`
                            });
                            parts.push({
                                inline_data: {
                                    mime_type: 'image/jpeg',
                                    data: frameImg.src.split(',')[1]
                                }
                            });
                        });

                        return { promptText, parts };
                    }

                    function downloadPrompt() {
                        try {
                            const { promptText, parts } = preparePromptData();

                            const blob = new Blob([JSON.stringify(parts, null, 2)], {type: 'application/json'});
                            const url = URL.createObjectURL(blob);
                            const a = document.createElement('a');
                            a.href = url;
                            a.download = 'prompt.json';
                            document.body.appendChild(a);
                            a.click();
                            document.body.removeChild(a);
                            URL.revokeObjectURL(url);
                        } catch (error) {
                            const errorDiv = document.getElementById('gvl-error');
                            errorDiv.style.display = 'block';
                            errorDiv.textContent = `Error: ${error.message}`;
                        }
                    }
                    
                    function shuffleArray(array) {
                        for (let i = array.length - 1; i > 0; i--) {
                            const j = Math.floor(Math.random() * (i + 1));
                            [array[i], array[j]] = [array[j], array[i]];
                        }
                        return array;
                    }
            
                    function orderFrames(orderType) {
                        const frameElements = Array.from(frameContainer.children);
                        frameElements.sort((a, b) => {
                            const aNum = parseInt(orderType === 'gt' ? 
                                a.querySelector('.gvl-frame-number').textContent :
                                a.querySelector('.gvl-shuffled-frame-number').textContent);
                            const bNum = parseInt(orderType === 'gt' ? 
                                b.querySelector('.gvl-frame-number').textContent :
                                b.querySelector('.gvl-shuffled-frame-number').textContent);
                            return aNum - bNum;
                        });
                        
                        frameContainer.innerHTML = '';
                        frameElements.forEach(frame => frameContainer.appendChild(frame));
                        isOrdered = orderType === 'gt';
                        
                        // Enable/disable buttons based on order
                        submitBtn.disabled = isOrdered;
                        parseBtn.disabled = isOrdered;
                        downloadBtn.disabled = isOrdered;
                    }
                    
                    function extractJSON(text) {
                        const codeBlockMatch = text.match(/```(?:json)?\s*([\s\S]*?)\s*```/);
                        if (codeBlockMatch) {
                            return codeBlockMatch[1];
                        }
            
                        const arrayMatch = text.match(/\[\s*{[\s\S]*}\s*\]/);
                        if (arrayMatch) {
                            return arrayMatch[0];
                        }
            
                        return null;
                    }
            
                    function parseResponse() {
                        const resultDiv = document.getElementById('gvl-result');
                        const errorDiv = document.getElementById('gvl-error');
                        const responseText = resultDiv.textContent;
            
                        if (isOrdered) {
                            errorDiv.style.display = 'block';
                            errorDiv.textContent = 'Frames must be in shuffled order to parse response';
                            return;
                        }
            
                        try {
                            const jsonText = extractJSON(responseText);
                            if (!jsonText) {
                                throw new Error('No valid JSON found in response');
                            }
            
                            const parsedFrames = JSON.parse(jsonText);
                            parsedFrames.forEach(frame => {
                                const frameEl = frameContainer.children[frame.frame_number - 1];
                                if (frameEl) {
                                    const progressFill = frameEl.querySelector('.gvl-frame-progress-fill');
                                    const description = frameEl.querySelector('.gvl-frame-description');
                                    progressFill.style.height = `${frame.task_completion_percentage}%`;
                                    description.textContent = frame.frame_description;
                                }
                            });
            
                            // Create plot data
                            const sortedFrames = [...parsedFrames].sort((a, b) => {
                                const aGt = parseInt(frameContainer.children[a.frame_number - 1].querySelector('.gvl-frame-number').textContent);
                                const bGt = parseInt(frameContainer.children[b.frame_number - 1].querySelector('.gvl-frame-number').textContent);
                                return aGt - bGt;
                            });
            
                            const plotData = [{
                                x: sortedFrames.map((_, i) => i + 1),
                                y: sortedFrames.map(f => f.task_completion_percentage),
                                type: 'scatter',
                                mode: 'lines+markers',
                                name: 'Predicted Progress'
                            }];
            
                            const layout = {
                                title: 'Task Completion Progress',
                                xaxis: {
                                    title: 'GT Frame Order',
                                    tickmode: 'linear'
                                },
                                yaxis: {
                                    title: 'Gemini Predicted Value Function',
                                    range: [0, 100]
                                }
                            };
            
                            plotContainer.style.display = 'block';
                            Plotly.newPlot('gvl-plot-container', plotData, layout);
            
                            errorDiv.style.display = 'none';
                        } catch (e) {
                            errorDiv.style.display = 'block';
                            errorDiv.textContent = `Error parsing response: ${e.message}`;
                        }
                    }
                    
                    imageInput.addEventListener('change', async function(e) {
                        const file = e.target.files[0];
                        if (!file) return;
            
                        if (file.type.startsWith('video/')) {
                            // Handle video file
                            const video = document.createElement('video');
                            video.src = URL.createObjectURL(file);
                            
                            video.onloadedmetadata = function() {
                                frameContainer.innerHTML = ''; // Clear previous frames
                                frameContainer.style.display = 'block';
                                orderButtons.style.display = 'flex';
                                imagePreview.style.display = 'none';
                                plotContainer.style.display = 'none';
                                isOrdered = true; // Start in GT order
                                submitBtn.disabled = true;
                                parseBtn.disabled = true;
                                downloadBtn.disabled = true;
                                
                                const canvas = document.createElement('canvas');
                                const context = canvas.getContext('2d');
                                
                                // Calculate frame count based on video duration
                                let frameCount;
                                let frameInterval;
                                if (video.duration <= 30) {
                                    frameCount = Math.floor(video.duration);
                                    frameInterval = 1;
                                } else {
                                    frameCount = 30;
                                    frameInterval = video.duration / 30;
                                }
                                
                                // Create shuffled frame numbers
                                const shuffledNumbers = shuffleArray([...Array(frameCount)].map((_, i) => i + 1));
                                
                                canvas.width = video.videoWidth;
                                canvas.height = video.videoHeight;
                                
                                let currentFrame = 0;
                                video.currentTime = 0;
                                
                                video.onseeked = function() {
                                    context.drawImage(video, 0, 0);
                                    const frameWrapper = document.createElement('div');
                                    frameWrapper.className = 'gvl-frame-container';
                                    
                                    const frameImg = document.createElement('img');
                                    frameImg.src = canvas.toDataURL('image/jpeg');
                                    frameImg.style.width = '150px';
                                    frameImg.style.margin = '5px';
                                    frameImg.onclick = function() {
                                        imageBase64 = frameImg.src.split(',')[1];
                                        imageType = 'image/jpeg';
                                    };
                                    
                                    const frameNumber = document.createElement('div');
                                    frameNumber.className = 'gvl-frame-number';
                                    frameNumber.textContent = `${currentFrame + 1}`;
                                    
                                    const shuffledFrameNumber = document.createElement('div');
                                    shuffledFrameNumber.className = 'gvl-shuffled-frame-number';
                                    shuffledFrameNumber.textContent = `${shuffledNumbers[currentFrame]}`;
                                    
                                    const progressBar = document.createElement('div');
                                    progressBar.className = 'gvl-frame-progress';
                                    const progressFill = document.createElement('div');
                                    progressFill.className = 'gvl-frame-progress-fill';
                                    progressBar.appendChild(progressFill);
                                    
                                    const description = document.createElement('div');
                                    description.className = 'gvl-frame-description';
                                    
                                    frameWrapper.appendChild(frameImg);
                                    frameWrapper.appendChild(frameNumber);
                                    frameWrapper.appendChild(shuffledFrameNumber);
                                    frameWrapper.appendChild(progressBar);
                                    frameWrapper.appendChild(description);
                                    frameContainer.appendChild(frameWrapper);
                                    
                                    currentFrame++;
                                    if (currentFrame < frameCount) {
                                        video.currentTime = currentFrame * frameInterval;
                                    }
                                };
                            };
                        } else {
                            // Handle image file
                            frameContainer.style.display = 'none';
                            orderButtons.style.display = 'none';
                            plotContainer.style.display = 'none';
                            imageType = file.type;
                            const reader = new FileReader();
                            reader.onload = function(e) {
                                imagePreview.src = e.target.result;
                                imagePreview.style.display = 'block';
                                imageBase64 = e.target.result.split(',')[1];
                            };
                            reader.readAsDataURL(file);
                        }
                    });

                    async function sendRequest() {
                        const apiKey = document.getElementById('gvl-api-key').value;
                        const resultDiv = document.getElementById('gvl-result');
                        const errorDiv = document.getElementById('gvl-error');
                        const progressDiv = document.getElementById('gvl-progress');

                        if (!apiKey) {
                            errorDiv.style.display = 'block';
                            errorDiv.textContent = 'Please provide API key';
                            return;
                        }

                        errorDiv.style.display = 'none';
                        resultDiv.textContent = '';
                        progressDiv.style.display = 'block';
                        submitBtn.disabled = true;

                        try {
                            const { parts } = preparePromptData();

                            const response = await fetch(
                                `https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro:generateContent?key=${apiKey}`,
                                {
                                    method: 'POST',
                                    headers: {
                                        'Content-Type': 'application/json',
                                    },
                                    body: JSON.stringify({
                                        contents: [{
                                            parts: parts
                                        }]
                                    })
                                }
                            );

                            const data = await response.json();

                            if (!response.ok) {
                                throw new Error(data.error?.message || 'API request failed');
                            }

                            const responseText = data.candidates?.[0]?.content?.parts?.[0]?.text || 'No response';
                            resultDiv.textContent = responseText;

                        } catch (error) {
                            errorDiv.style.display = 'block';
                            errorDiv.textContent = `Error: ${error.message}`;
                            resultDiv.textContent = '';
                        } finally {
                            progressDiv.style.display = 'none';
                            submitBtn.disabled = isOrdered;
                        }
                    }
                </script>
            </body>
            </div>
          </div>
        </div>
      </section>  

    
    <section class="section">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column is-full-width has-text-justified">
            <h2 class="title is-3 has-text-centered">Acknowledgement</h2>
            <div class="content has-text-justified">
              <p>
                We thank Jie Tan, Pannag Sanketi, Oliver Groth, and the rest the Google DeepMind Robotics team for helpful discussions and providing feedback on the paper.              </p>
            </div>
          </div>
        </div>
      </div>
    </section>

    <div class="hr"></div>

    <section class="section" id="BibTeX">
      <div class="container is-max-desktop content">
        <h2 class="title has-text-centered">BibTeX</h2>
        <pre><code>@preprint{ma2024generative,
    author    = {Yecheng Jason Ma and Joey Hejna and Ayzaan Wahid and Chuyuan Fu and Dhruv Shah and Jacky Liang and Zhuo Xu and Sean Kirmani and Peng Xu and Danny Driess and Ted Xiao and Jonathan Tompson and Osbert Bastani and Dinesh Jayaraman and Wenhao Yu and Tingnan Zhang and Dorsa Sadigh and Fei Xia},
    title     = {Vision Language Models are In-Context Value Learners},
    booktitle = {preprint},
    year      = {2024}
}</code></pre>


    </section>
  </body>
</html>
